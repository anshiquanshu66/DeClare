{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from data.data import DeClareDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOPES_LOC = \"./Datasets/Snopes/snopes.tsv\"\n",
    "#consists of rumors analyzed on the Snopes website along with their credibility labels (true or false), \n",
    "#sets of reporting articles, and their respective web sources\n",
    "\n",
    "POLITIFACT_LOC = \"./Datasets/PolitiFact/politifact.tsv\"\n",
    "\n",
    "glove_data_file = \"./Glove/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read news data from ./Datasets/Snopes/snopes.tsv\n",
      "Number of articles = 29242\n",
      "Number of claims = 4341\n",
      "Using pre-built vocabulary\n"
     ]
    }
   ],
   "source": [
    "snopes = DeClareDataset(SNOPES_LOC, glove_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Credibility</th>\n",
       "      <th>Claim_Source</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Article</th>\n",
       "      <th>Article_Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>politics_christmas_bestbuy</td>\n",
       "      <td>best buy chain eschewing use word christmas 20...</td>\n",
       "      <td>there are several holidays throughout that tim...</td>\n",
       "      <td>www.godlikeproductions.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true</td>\n",
       "      <td>politics_christmas_bestbuy</td>\n",
       "      <td>best buy chain eschewing use word christmas 20...</td>\n",
       "      <td>defenses against same photographs show images ...</td>\n",
       "      <td>www.sjpba.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true</td>\n",
       "      <td>politics_christmas_bestbuy</td>\n",
       "      <td>best buy chain eschewing use word christmas 20...</td>\n",
       "      <td>the best that life could think out we extended...</td>\n",
       "      <td>www.englisher.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>true</td>\n",
       "      <td>politics_christmas_bestbuy</td>\n",
       "      <td>best buy chain eschewing use word christmas 20...</td>\n",
       "      <td>this entry november 19 2006 published 9 years ...</td>\n",
       "      <td>rss2.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>true</td>\n",
       "      <td>politics_christmas_bestbuy</td>\n",
       "      <td>best buy chain eschewing use word christmas 20...</td>\n",
       "      <td>place he will not do either said snape the ord...</td>\n",
       "      <td>www.englisher.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Credibility                Claim_Source  \\\n",
       "0        true  politics_christmas_bestbuy   \n",
       "1        true  politics_christmas_bestbuy   \n",
       "2        true  politics_christmas_bestbuy   \n",
       "3        true  politics_christmas_bestbuy   \n",
       "4        true  politics_christmas_bestbuy   \n",
       "\n",
       "                                               Claim  \\\n",
       "0  best buy chain eschewing use word christmas 20...   \n",
       "1  best buy chain eschewing use word christmas 20...   \n",
       "2  best buy chain eschewing use word christmas 20...   \n",
       "3  best buy chain eschewing use word christmas 20...   \n",
       "4  best buy chain eschewing use word christmas 20...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  there are several holidays throughout that tim...   \n",
       "1  defenses against same photographs show images ...   \n",
       "2  the best that life could think out we extended...   \n",
       "3  this entry november 19 2006 published 9 years ...   \n",
       "4  place he will not do either said snape the ord...   \n",
       "\n",
       "               Article_Source  \n",
       "0  www.godlikeproductions.com  \n",
       "1               www.sjpba.net  \n",
       "2           www.englisher.net  \n",
       "3                    rss2.com  \n",
       "4           www.englisher.net  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snopes.news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeClareModel(nn.Module):\n",
    "    def __init__(self, glove_embeddings, claim_source_vocab_size, article_source_vocab_size):\n",
    "        super(DeClareModel, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(glove_embeddings), freeze=False)\n",
    "        self.claim_source_embeddings = nn.Embedding(claim_source_vocab_size, claim_source_vocab_size)\n",
    "        self.article_source_embeddings = nn.Embedding(article_source_vocab_size, article_source_vocab_size)\n",
    "        \n",
    "    def forward(self, claim, article, claim_source, article_source):\n",
    "        claim_mean_embedding = torch.mean(self.word_embeddings(claim), 0)\n",
    "        article_embeddings = self.word_embeddings(article)\n",
    "        \n",
    "        demo_output = torch.cat((claim_mean_embedding.view(1,-1), article_embeddings), 0)\n",
    "        return demo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "declare = DeClareModel(snopes.initial_embeddings, snopes.claim_source_vocab_size, snopes.article_source_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1210e-03,  1.0612e-01, -1.7659e-01,  ..., -1.0494e-03,\n",
      "          6.3730e-01,  2.3561e-01],\n",
      "        [-8.4215e-02,  6.9597e-01,  2.8383e-01,  ..., -5.4000e-01,\n",
      "          5.8031e-01,  4.6698e-01],\n",
      "        [-5.1533e-01,  8.3186e-01,  2.2457e-01,  ..., -1.2024e+00,\n",
      "          1.1304e+00,  3.4790e-01],\n",
      "        ...,\n",
      "        [-4.6851e-01,  2.6150e-01, -3.2521e-01,  ..., -5.1552e-01,\n",
      "          7.6918e-01, -1.3301e-01],\n",
      "        [-2.8111e-01, -6.0057e-01, -8.3444e-02,  ..., -3.1976e-01,\n",
      "         -3.7547e-01,  4.3759e-01],\n",
      "        [-5.9779e-02,  2.5019e-01, -2.0438e-01,  ...,  4.0870e-01,\n",
      "          7.4594e-02, -1.6164e-01]], dtype=torch.float64,\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "for data_sample in snopes:\n",
    "    print(declare(data_sample[0], data_sample[1], data_sample[2], data_sample[3]))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
